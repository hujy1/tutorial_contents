{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Land Classification using Supervised Machine Learning in 10-ish Steps\n",
    "\n",
    "## Goal:\n",
    "\n",
    "<i>In this tutorial, you will work with nadir-looking imagery to run supervised machine learning models to perform land classification. From this excersize, you will:\n",
    "\n",
    "1. gain insight into setting up a geospatial machine learning model,\n",
    "1. understand differences in imagery types,\n",
    "1. create testing and training data for supervised modelling,\n",
    "1. evalaute performance against multiple model types,\n",
    "1. gain knowledge of streaming pixels from the cloud. </i>\n",
    "\n",
    "## Background\n",
    "\n",
    "Land classification is the technique of labelling each individual pixel in an image with its relevant class (e.g. water, road, tree, etc). In remote sensing, there is a long history of this process, largely driven by manual labor. With the rise of increased acquisition from digital sensor platforms, at high resolution, manual classification is unscalabile and can have inherent human biases. Machine learning is ideal for land classification in its ability to scale the pixel-wise labelling exponentially. A Support Vector Machine implementation is very straightfoward, and is discusssed here. However, for large-scale processing workflows, Convolutional Neural Networks (CNN) have become ideal.\n",
    "\n",
    "![alt-text](lc.png \"Logo Title Text 1\")\n",
    "\n",
    "<i>Figure 1. RGB overhead imagery (left) is used in combination with truth data to perform a pixel-size land classification (far right), where each pixel is translated from an RGB value to a label class. In this case, the classes are pool (cyan), impervious surface (grey), tree/shrub (dark green), irrigated lawn (light green), shadows (dark grey), natural/non-irrirgated lawn (brown). This land classification map was produced using a deep learning Convolutional Neural Network.</i>\n",
    "\n",
    "### 1. <b><u>To start</u></b>, get the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6c2fa1faa2b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUNSIGNED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "# Get all the libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import os\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define some functions. \n",
    "\n",
    "We will define some functions that can read an image from cloud storage on Amazon S3 and stream it into memory and also a few plotting function for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream2npy(img_stream):\n",
    "    '''convert image in-memory stream to an numpy array'''\n",
    "    arr = np.asarray(bytearray(img_stream['Body'].read()), dtype=np.uint8)\n",
    "    return arr\n",
    "\n",
    "def statsdata(arr):\n",
    "    '''generate histogram of training data'''\n",
    "    fig=plt.figure(figsize=(7, 7))\n",
    "    bins = range(8)\n",
    "    plt.hist(arr, bins=bins)  # arguments are passed to np.histogram\n",
    "    bins_labels(bins, fontsize=20)\n",
    "    plt.title('Distribution of Training Data Classes')\n",
    "    plt.xlabel('Classes')\n",
    "\n",
    "def bins_labels(bins, **kwargs):\n",
    "    '''center the histogram bin labels due to OCD'''\n",
    "    bin_w = (max(bins) - min(bins)) / (len(bins) - 1)\n",
    "    plt.xticks(np.arange(min(bins)+bin_w/2, max(bins), bin_w), bins, **kwargs)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Grab data from the cloud.\n",
    "\n",
    "We will grab two sets of data from an S3 bucket in the form of a geotif. You can also download these locally to play with later (```s3.Bucket(BUCKET_NAME).download_file(KEY_DG, '/my_local_path/'+KEY_DG)```). For now, we will just stream the pixel data into memory for processing. Evaluate the print statements. \n",
    "\n",
    "<span style=\"color:fuchsia\"><b>Question:</b></span> What type of information do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the imagery from AWS S3 to local memory\n",
    "\n",
    "BUCKET_NAME = 'uw-geohack' # replace with your bucket name\n",
    "KEY_DG = 'la_digitalglobe_small.tif' \n",
    "KEY_EV = 'la_eagleview_small.tif' \n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3',config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "img_stream_dg = s3_client.get_object(Bucket=BUCKET_NAME, Key=KEY_DG)\n",
    "img_stream_ev = s3_client.get_object(Bucket=BUCKET_NAME, Key=KEY_EV)\n",
    "\n",
    "print('DG:', img_stream_dg)\n",
    "print('EV: ',img_stream_ev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into memory and convert it to a numpy array or pixels in RGB space\n",
    "img1=stream2npy(img_stream_dg)\n",
    "img1_de = cv2.imdecode(img1, -1)\n",
    "img1_rgb = cv2.cvtColor(img1_de, cv2.COLOR_BGR2RGB)   #Why do we need to do this? What is this doing?\n",
    "\n",
    "img2=stream2npy(img_stream_ev)\n",
    "img2_de = cv2.imdecode(img2, -1)\n",
    "img2_rgb = cv2.cvtColor(img2_de, cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Let's visualize the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(18, 16))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img2_rgb)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img1_rgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:fuchsia\"><b>Question:</b></span> One image is satellite imagery and one is aerial imagery. Can you tell which is which? Why? What other observations can you make about these images?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Get the training data too\n",
    "\n",
    "We downloaded the RGB imagery for the aerial and satellite images. Now we need to get the two corresponding training data sets. \n",
    "\n",
    "<span style=\"color:fuchsia\"><b>Question:</b></span> Why are there two different data sets? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EV = 'training_data_ev.tif'\n",
    "TRAIN_DG = 'training_data_dg.tif'\n",
    "\n",
    "\n",
    "train_stream = s3_client.get_object(Bucket=BUCKET_NAME, Key=TRAIN_EV)\n",
    "train=stream2npy(train_stream)\n",
    "train_de = cv2.imdecode(train, -1)\n",
    "\n",
    "train_stream2 = s3_client.get_object(Bucket=BUCKET_NAME, Key=TRAIN_DG)\n",
    "train2=stream2npy(train_stream2)\n",
    "train2_de = cv2.imdecode(train2, -1)\n",
    "\n",
    "print('Shape of the Training data:', train_de.shape)\n",
    "print('Shape of the Image data:', img2_rgb.shape)\n",
    "\n",
    "print('Shape of the Training data:', train2_de.shape)\n",
    "print('Shape of the Image data:', img1_rgb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:fuchsia\"><b>Question:</b></span> What do you notice about the training and image data shapes? Do they make sense?  \n",
    "\n",
    "### 6. So we cheated... (how to make more training data if you need) \n",
    "\n",
    "I hooked you up with the training data. But (as you may discover soon) you will want to add, modify, or change the training data. \n",
    "\n",
    "<b><u>Creating more training data:</u></b> The training data was created in QGIS as a geojson vector layer on a local machine. This layer was rasterized using the command line [rasterio tools](https://github.com/mapbox/rasterio) like so:\n",
    "\n",
    "```rio rasterize training_data.tif --like la_eagleview_small.tif --property id --fill 0 < training_data.geojson```\n",
    "\n",
    "To use these tools, clone the rasterio repo and use on the local command line.\n",
    "\n",
    "The string `id` is the attribute name of the class you labeled with a polygon and `--fill` will fill all no-data areas with the `0` value.\n",
    "\n",
    "<b><u>Saving local copies of the images:</b></u> The training data geojson and the image have to both be in the same CRS, of course. And you'll want to save the streaming (in-memory) images to your local machine:\n",
    "\n",
    "```s3.Bucket(BUCKET_NAME).download_file(KEY_DG, '/my_local_path/'+KEY_DG)```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Let's evaluate the training data for the satellite image.\n",
    "\n",
    "The training data array has values corresponding to this key of classes. Here is your key:\n",
    "\n",
    "`0 - unclassified, 1 - pool, 2 - street, 3 - grass, 4 - roof, 5 - tree, 6 - shadow`\n",
    "\n",
    "But let's double check on the contents of the training data. For now, we are just going to look at one set of training data due to time constraints. But the same technique we use here can be applied to the other dataset. Let's use the smaller dataset, `train_de`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {'pool': 1, 'street': 2, 'grass': 3, 'roof': 4, 'tree': 5, 'shadow': 6}\n",
    "n_classes = len(classes)\n",
    "print('Unique values in training array: ',np.unique(train_de))\n",
    "\n",
    "# create a color palette we will use to colorize the predictions later\n",
    "palette = np.uint8([[0, 0, 0],[0, 255, 255], [128, 128, 128], [0, 255, 0],[255, 255, 255],[0, 102, 0],[51, 51, 51]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:fuchsia\"><b>Question:</b></span> What about the distribution of class types? Let's visualize that as well in order to understand more about our data set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsdata(train_de[train_de>0].ravel())\n",
    "statsdata(train2_de[train2_de>0].ravel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:fuchsia\"><b>Question:</b></span> What could the distribution of the training data indicate? \n",
    "What changes could be useful?\n",
    "\n",
    "### 8. Let's create a training data mask.\n",
    "\n",
    "We will need to mask out the parts of the RGB image we wont be using. If we don't that will create confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols, bands = img1_rgb.shape\n",
    "full=img1_rgb[:,:,0:3]        #Why do we do this?\n",
    "full=full.ravel()\n",
    "full=full.reshape((-1, 1))    #Set this guy aside for full prediction\n",
    "\n",
    "red=img1_rgb[:,:,0]\n",
    "green=img1_rgb[:,:,1] \n",
    "blue=img1_rgb[:,:,2]\n",
    "\n",
    "# remove all the 'class 0' from the training data\n",
    "red=np.where(train2_de>0, red,0)\n",
    "green=np.where(train2_de>0, green,0)\n",
    "blue=np.where(train2_de>0, blue,0)\n",
    "\n",
    "# create a mask with the same dimensions\n",
    "Xtrain=np.dstack((red,green,blue))\n",
    "Ylabel=np.dstack((train2_de,train2_de,train2_de))\n",
    "\n",
    "# flatten\n",
    "data = Xtrain.ravel()     \n",
    "label= Ylabel.ravel()  \n",
    "\n",
    "# remove all the 'class 0' from the training data\n",
    "l=label[label>0]\n",
    "d=data[label>0]\n",
    "d=d.reshape((-1, 1)) \n",
    "\n",
    "\n",
    "plt.imshow(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:fuchsia\"><b>Question: </b></span>What do you notice about this image above?\n",
    "\n",
    "### 9. Training the SVM\n",
    "\n",
    "What is a Support Vector Machine (SVM)? Given a set of labeled training data (supervised learning), the SVM outputs an optimal hyperplane which categorizes new data (not used in training). For a simple case of two classes in 2 dimensional space, the hyperplane is a line dividing a plane in two parts where each of the 2 classes lay on either side. SVMs are useful for classification problems where you would like to different among mulitple classes. However, as we will see, it can fall short due to scalability and processing overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# split the training data up so we can test later\n",
    "X_train, X_test, y_train, y_test = train_test_split(d, l, test_size=0.25)\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_t = clf.predict(full)\n",
    "predicted=y_t.reshape(rows, cols,3)\n",
    "\n",
    "fig=plt.figure(figsize=(18, 16))\n",
    "plt.imshow(palette[predicted][:,:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:fuchsia\"><b>Question: </b></span>What do you notice about the results? How could we improve them?\n",
    "\n",
    "### 10. What is the model's performance?\n",
    "\n",
    "Let's create a confusion matrix with the test data we held back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm, metrics\n",
    "expected = y_test\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (clf, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:fuchsia\"><b>Question: </b></span> How can we define accuracy? Precision? What is recall?\n",
    "\n",
    "\n",
    "### 11. Train another type of ML model\n",
    "\n",
    "Let's try out a random forest classifier. What is a random forest (RF) classifier? An RF classifier constructs  decision trees during supervised training and outputs the class that is the mode of the classification of the individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_rf = clf.predict(full)\n",
    "predictedRF=y_rf.reshape(rows, cols,3)\n",
    "\n",
    "fig=plt.figure(figsize=(18, 16))\n",
    "plt.imshow(palette[predictedRF][:,:,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. What about the other image?\n",
    "\n",
    "What happens when we apply the learned model to the aerial imagery with the higher resolution? Try is out using the above recipe. It may take a while given the amount of pixels in the aerial image.\n",
    "\n",
    "<span style=\"color:fuchsia\"><b>Question: </b></span>What kind of differences do you expect? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
